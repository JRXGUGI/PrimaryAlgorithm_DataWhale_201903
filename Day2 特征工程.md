## 1 特征衍生
> 特征衍生在本身特征提取之上进行，主要分为自有特征变换、特征创造、映射、交叉、生维


### 1.1 自有特征变换
该部分有很多方法，不同的方法适用于不同的业务场景      


1. 不同阶的差分     
2. 傅立叶变换  
3. 多项式做组合特征（二次三次等）（sklearn.preprocessing.PolynomialFeatures）
4. 但任何针对单独特征列的单调变换（如对数 ，平方，立方，根号...）
5. 线性组合：仅适用于决策树以及基于决策树的ensemble（如gradient boosting, random forest）
6. 比例特征：特征之间的加减乘除等
7. 类别特征与数值特征的组合：用N1和N2表示数值特征，用C1和C2表示类别特征，利用pandas的groupby操作，可以创造出以下几种有意义的新特征：（其中，C2还可以是离散化了的N1）

		> median(N1)_by(C1)  \\ 中位数 
		> mean(N1)_by(C1)  \\ 算术平均数
		> mode(N1)_by(C1)  \\ 众数
		> min(N1)_by(C1)  \\ 最小值
		> max(N1)_by(C1)  \\ 最大值
		> std(N1)_by(C1)  \\ 标准差
		> var(N1)_by(C1)  \\ 方差
		> freq(C2)_by(C1)  \\ 频数
		> N1 - median(N1)_by(C1)
		> N1 - mean(N1)_by(C1)
	
### 1.2 创造新特征
> 主要是使用决策树、线性组合等方法

在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector作为一项新的特征，加入到模型中。      

具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。
	
### 1.3 Histogram映射：
	
把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。 

例如统计“性别与爱好的关系”，性别有“男”“女”，爱好有三种，表示成向量[散步、足球、看电视剧]，分别计算男性和女性中每个爱好的比例得到：男[1/3, 2/3, 0]，女[0, 1/3, 2/3]。即反映了两个特征的关系。
	
### 1.4 特征交叉
特征交叉对于线性模型可以学习到非线性特征。例如两个特征：年龄和性别，可以组合成 年龄_性别 的一个新特征，比如M_18，F_22等等，然后再对这个特征做one hot编码，即可得到新的特征属性值。   

> 至于交叉特征的新特征向量是不是原特征向量的内积，答案是否。因为原特征也是one hot的离散变量，长度不一定相等，如果做向量内积是0。所以正确的做法是，先用原语义做简单字符串拼接，然后再做onehot编码。不过，暴力做交叉特征可能产生的稀疏的问题，这就是另一个问题了。可以参考FM和FFM的解决方案，LIBFFM的库，以及阿里妈妈发布的MLR算法。

### 1.5 特征升维
- 核方法
- autoencoder
- CNN

但是，并不是特征构造越多就效果越好,有两个原因：    
1. 特征越多，越容易过拟合；       
2. 特征越多，数据越稀疏，需要相应增加的训练数据成指数级增长。     

--- 

### 2 特征选择 
> 特征的选择一般需要考虑两点：   
1. 特征是否发散：如果一个特征不发散，就是说这个特征大家都有或者非常相似，说明这个特征不需要。     
2. 特征和目标是否相关：与目标的相关性越高，越应该优先选择。    

按照特征评价标准分类：

* 选择使分类器的错误概率最小的特征或者特征组合。
* 利用距离来度量样本之间相似度。
* 利用具有最小不确定性的那些特征来分类。
* 利用相关系数, 找出特征和类之间存在的相互关系；
* 利用特征之间的依赖关系, 来表示特征的冗余性加以去除。


#### 2.1 过滤法Filter
* 方差选择法:计算各个特征方差，选择方差大于阈值的特征；

* 相关系数法:计算各个特征的Pearson相关系数

* 信息熵:计算各个特征的信息增益

* Chi-Square：卡方检验

> 就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合。


#### 2.2 封装法Wrapper
* 递归消除法:使用基模型(如LR)在训练中进行迭代，选择不同特征
* 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征
* 前向选择法：从0开始不断向模型加能最大限度提升模型效果的特征数据用以训练，直到任何训练数据都无法提升模型表现。
* 后向剃除法：先用所有特征数据进行建模，再逐一丢弃贡献最低的特征来提升模型效果，直到模型效果收敛。


#### 2.3 嵌入法Embedded(效果最好速度最快，模式单调，快速并且效果明显， 但是如何参数设置， 需要深厚的背景知识。)
* 使用带惩罚项的基模型进行特征选择     

> 比如LR加入正则。通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验
			
* 树模型的特征选择(随机森林、决策树)   

> 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；  

* Lasso
* Elastic Net
* Ridge Regression


#### 2.4  特征降维
> 该部分涉及较少，以后需要注意补充

- 主成分分析(PCA):选择方差最大的K个特征
- 线性判别分析(LDA):选择分类性能最好的特征

### 3 使用IV值和决策森林进行特征挑选
网上有相关的文档[用IV值和随机森林挑选特征](https://blog.csdn.net/zhangyunpeng0922/article/details/84591046)，因为时间关系，先给出地址，做特征工程知识梳理的时候一并总结。


#### 参考资料
* [机器学习之特征工程](https://www.cnblogs.com/wxquare/p/5484636.html)
* [使用sklearn做单机特征工程 ](http://www.cnblogs.com/jasonfreak/p/5448385.html)
* [特征选择与特征抽取](https://blog.csdn.net/google19890102/article/details/40019271)
* [使用sklearn优雅地进行数据挖掘 ](http://www.cnblogs.com/jasonfreak/p/5448462.html#3955242)